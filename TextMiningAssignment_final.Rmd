---
title: "TextMining"
author: "Omar Elhayboubi, Eduardo Herrera, Antonio Cruz"
Date: 04/15/2021
output:
  pdf_document: default
  html_document: default
---

```{r message=FALSE, warning=FALSE, include=FALSE}
#Calling libraries
library(tidyverse)
library(tidytext)
library(SnowballC)
library(textstem)
library(rsample)
library(ranger)
library(pROC)
library(e1071)
library(caret)
```


## (a) Explore the data. 

In this section, we will explore the data in coming from Yelp reviews.

Lets look at how star ratings are distributed.

```{r message=FALSE, warning=FALSE}
#read the Yelp ratings CSV file
resReviewsData <- read.csv2("yelpRestaurantReviews_sample.csv")
#number of reviews by star rating
star_dist <- resReviewsData %>% group_by(stars) %>% count()
#plot the distribution
ggplot(star_dist, aes(x= stars, y=n)) +geom_bar(stat="identity")

```

We could see the distribution of star ratings in the plot above, we could see that as star ratings increase, number of stars increases as well.
In order to obtain a label indicating 'positive' or 'negative' using star ratings, we will label reviews that have star ratings < 3 as negative and the reviews that have star ratings > 3 as positive.

Now lets analyze the reviews that were labeled as 'funny', 'cool', 'useful' and see how they relate to the star ratings.

```{r message=FALSE, warning=FALSE}
#funny
ggplot(resReviewsData, aes(x= funny, y=stars)) +geom_point()
#cool
ggplot(resReviewsData, aes(x= cool, y=stars)) +geom_point()
#Useful
ggplot(resReviewsData, aes(x= useful, y=stars)) +geom_point()
```

In the graphs above, we see that the reviews that were labeled as 'funny', 'cool', 'useful' are more present in the higher rated reviews but not as much as we expected. from these graphs I could conclude that if a review was labled as cool or useful for example, this does not necessarily mean that this review was positive.

We could also explore how the reviews are distributed by location.

```{r message=FALSE, warning=FALSE}
resReviewsData %>% group_by(state) %>% tally() %>% view()
```

Most of the reviews are from the US, but we do see some reviews are coming from outside the US.

We could use the code below to only keep the reviews from the US

```{r message=FALSE, warning=FALSE}
rrData <- resReviewsData %>% filter(str_detect(postal_code, "^[0-9]{1,5}"))
rrData %>% group_by(state) %>% tally() %>% view()
```


## (b) What are some words indicative of positive and negative sentiment?

In this section, we will explore if the usage of some words indicates positive and negative sentiments.

### Some Data Preperation

In order to do this, we will first tokenize the text of the reviews in the column named text. We will only keep the reviewID and star attributes. We will also remove the stop_words.

```{r message=FALSE, warning=FALSE}
#tokenize the text of the reviews in the column named 'text'
rrTokens <- rrData %>% unnest_tokens(word, text)
 
#only keep reviewID stars and text
rrTokens <- rrData %>% select(review_id, stars, text ) %>% unnest_tokens(word, text)

#remove stopwords
rrTokens <- rrTokens %>% anti_join(stop_words)

dim(rrTokens)

#display the results
head(rrTokens)
```

We could see from the results above that after we tokenized the text of the reviews, now we are able to look at each word with its star rating which will help us analyze if some words indicate postive or negative sentiments.

Now, we will plot the most frequent words by star ratings and analyze the results

```{r message=FALSE, warning=FALSE}
#Most frequent words
rrTokens %>% count(word, sort=TRUE) %>% top_n(10)
```

In the table above, we could see the top10 most frequently used words in the reviews.

Now lets look at the most rare words that are used in a very small number of reviews and remove them.

```{r message=FALSE, warning=FALSE}
#Look for words used less than 10 times
rareWords <-rrTokens %>% count(word, sort=TRUE) %>% filter(n<10)
#remove these rare words
xx<-anti_join(rrTokens, rareWords)

#filter by rare words
xx2<- xx %>% filter(str_detect(word,"[0-9]")==FALSE)
#put filtered data back into rrTokens
rrTokens<- xx2
```

### Analyze words/Sentiments Based on Star Ratings

Now after some data preparation, we could analyze words and how they indicate wether a review is positive or negative

```{r message=FALSE, warning=FALSE}
#proportion of word occurrence by star ratings
ws <- rrTokens %>% group_by(stars) %>% count(word, sort=TRUE)
ws<-  ws %>% group_by(stars) %>% mutate(prop=n/sum(n))

#To plot this
ws %>% filter(! word %in% c('food', 'time', 'restaurant', 'service', 'chicken'))%>% 
  group_by(stars) %>% arrange(stars, desc(prop)) %>% 
  filter(row_number() <=10) %>% 
  ggplot(aes(word, prop))+geom_col()+coord_flip()+facet_wrap((~stars))

```

In the plot above, I remove some words like food, time, restaurant, service as they are very frequent don't really help in our use case.

The plot above makes sense. We see that words like worst and bad are only found in the lower star ratings, while words like amazing, delicious, pretty are found in the higher star ratings. We do have some words that are found in all star ratings like pizza, minutes as these words could be used in both positive and negative review and doesn't make sense to use these as way to help us determine positive/negative sentiments

Now in order to get sense of which words are related to higher/lower ratings ('positive', 'negative' sentiments), we will calculate the average star rating associated with each word sum(stars*prop). Then we could look at the 20 words with highest ratings and the top 20 words with lowest ratings.

```{r message=FALSE, warning=FALSE}
#calculate
xx<- ws %>% group_by(word) %>% summarise(totWS=sum(stars*prop))
#top 20 words with highest rating and top 20 words with lowest ratings
xx %>% top_n(20)
xx %>% top_n(-20)
```

A lot of the words in the results above do make sense, like amazing, delicious, friendly, love for the higher ratings. However, some of the words here I did not expect like food, lunch, chicken (these words could be used in both high and lower star ratings).
We see a mix of words that we expected to see and others that do not make sense. Same thing on the lower ratings, some words make sense like (bullshit, disgust, disrespectful, patronizing) bit many words don't like fax, coffee, vehicle, triangle.

Now, we will try using TF-IDF (term-frequency, inverse document frequency) which is a measure that will help us see how important a words is to a document in a collection of documents. Calculation: n times a word appears in a doc, inverse document frequency of the word across a set of documents. we will also lemmitize the rrtoken the words in order to get the original form of each word using the lemmatization algorithm. 

```{r message=FALSE, warning=FALSE}
#lemmatizing words in rrtokens
rrTokens<-rrTokens %>%  mutate(word = textstem::lemmatize_words(word))
#filtering by words with character <=3 or <=15
rrTokens<-rrTokens %>% filter(str_length(word)<=3 | str_length(word)<=15)
#Count of words in each review in column n
rrTokens<- rrTokens %>% group_by(review_id, stars) %>% count(word)
#function to calculate TF-IDF
rrTokens<-rrTokens %>% bind_tf_idf(word, review_id, n)
head(rrTokens)

```




## (c) Working with Dictionaries (AFINN, NRC, BING)

In this section, we will use 3 sentiment dictionaries BING, NRC and AFINN to get sentiment analysis of the words used in each reviews. All 3 dictionaries following a different approach on how to label positive/negative. We will first start with the BING dictionary

### Bing Dictionary 

We will get the sentiments for all words in the dictionary, then perform an inner_join with rr_tokens to avoid the words that will get an NA value. The bing dictionary results in either 'Positive' or 'Negative' sentiment for each word. Some words will give us NA results, so we will use an inner join to avoid the na values.

```{r message=FALSE, warning=FALSE}
#Get sentiments inner join rrTokens
rrSenti_bing<- rrTokens %>% inner_join(get_sentiments("bing"), by="word")

#Make positive count for 'positive words' and negative count for negative words
xx<-rrSenti_bing %>% group_by(word, sentiment) %>% summarise(totOcc=sum(n)) %>%
  arrange(sentiment, desc(totOcc))
xx<- xx %>% mutate (totOcc=ifelse(sentiment=="positive", totOcc, -totOcc))

#the most positive and most negative words
xx<-ungroup(xx)
xx %>% top_n(25)
xx %>% top_n(-25)

#plot the results
rbind(top_n(xx, 25), top_n(xx, -25)) %>% mutate(word=reorder(word,totOcc)) %>%
  ggplot(aes(word, totOcc, fill=sentiment)) +geom_col()+coord_flip()
```

In the graph above, we could see the most frequent 'Positive words' and most frequent 'negative words' according to the Bing dictionary. These results make total sense to us. For example the most used negative word is bad and the most used positive word is love. 

### NRC Dictionary

Now lets see how NRC will label sentiments to the words in the reviews. NRC works differently than BING; It assigns different emotions to words: anger, anticipation, disgust, fear, joy, sadness, surprise, trust and sentiments 'positive', 'negative'. We will again get all the sentiments from this dictionary, perform an inner join with rrtokens and finally, we will try to convert the emotions to either negative or postive sentiments as follows:

  - Negative: 'anger', 'disgust', 'fear', 'sadness', 'negative'
  - Positive: 'positive', 'joy', 'anticipation', 'trust'
  
```{r message=FALSE, warning=FALSE}
#with "nrc" dictionary
rrSenti_nrc<-rrTokens %>% inner_join(get_sentiments("nrc"), by="word") %>%
  group_by (word, sentiment) %>% summarise(totOcc=sum(n)) %>%
  arrange(sentiment, desc(totOcc))
#split into only Positive and Negative sentiments
xx<-rrSenti_nrc %>% 
  mutate(goodBad=ifelse
         (sentiment %in% c('anger', 'disgust', 'fear', 'sadness', 'negative'),
           -totOcc, ifelse(sentiment %in% c('positive', 'joy', 'anticipation', 
                                            'trust'), totOcc, 0)))
#ungroup
xx<-ungroup(xx)
#Plot the results
rbind(top_n(xx, 25), top_n(xx, -25)) %>% 
  mutate(word=reorder(word,goodBad)) %>% 
  ggplot(aes(word, goodBad, fill=goodBad)) +geom_col()+coord_flip()

```

A lot of words here make sense like bad being the most frequent bad word; However, we do see some words that don't make total sense like Chicken, food, time; These words cold be used in both good and bad. Generally these results make sense but still are different than bing results. The Bing results make more sense to us.

### AFINN Dictionary 

Now, we will use the AFINN dictionary. Afinn assigns an integer between -5 for negative and +5 for positive. What we could is change all the negative words to 'Negative' and all positive words to 'positive' in order to analyze the results in the same manner we did with the two other dictionaries. 

```{r message=FALSE, warning=FALSE}
#with "afinn" dictionary
rrSenti_afinn<-rrTokens %>% inner_join(get_sentiments("afinn"), by="word") %>%
  group_by (word, value) %>% summarise(totOcc=sum(n)) %>%
  arrange(value, desc(totOcc))
#split into only Positive and Negative sentiments
xx<-rrSenti_afinn %>% 
  mutate(posNeg=ifelse
         (value < 0,
           -totOcc, ifelse(value > 0, totOcc, 0)))
#ungroup
xx<-ungroup(xx)
#Plot the results
rbind(top_n(xx, 25), top_n(xx, -25)) %>% 
  mutate(word=reorder(word,posNeg)) %>% 
  ggplot(aes(word, posNeg, fill=posNeg)) +geom_col()+coord_flip()
```

As you can see in the plot above, these results from Afinn make total sense and are very similar to the bing results.

To conclude, we see that each sentiment dictionary uses different methods to assign a sentiment to each word. We do see similarities between bing and afinn but not as much with NRC. Which makes sense because NRC use emotions and not just sentiments.

Based on these results, We will use the afinn and bing dictionary going forward.

### Perform analysis by review Ratings/Word Sentiment

In the previews section, we analyzed the sentiments for words based on the dictionaries. Now, we want to see how these sentiments relate to reviews (each review id) and it's star rating. This will give a good idea on how on how these dictionaries perform in determining whether a review is positive or negative. For Bing,  We could do this by calculating the number of positive words and negative words in each reviews, they divide them by the number of words in the review. Then, we could get the average sentiment score based on these proportions of pos and neg in each review.

Lets start with the bing dictionary

```{r message=FALSE, warning=FALSE}
revSenti_bing <- rrSenti_bing %>%
  group_by(review_id, stars) %>%
  summarise(nwords=n(),posSum=sum(sentiment=='positive'),
            negSum=sum(sentiment=='negative'))

revSenti_bing<- revSenti_bing %>%
  mutate(posProp=posSum/nwords, negProp=negSum/nwords)
revSenti_bing<- revSenti_bing %>%
  mutate(sentiScore=posProp-negProp)

#Do review start ratings correspond to the the positive/negative sentiment words
revSenti_bing %>% group_by(stars) %>%
  summarise(avgPos=mean(posProp), avgNeg=mean(negProp), avgSentiSc=mean(sentiScore)) %>%
  ggplot (aes(x= stars, y=avgSentiSc)) +geom_bar(stat="identity")

```

This makes total sense, as we could see in the table above, as the star ratings go up, the average sentiment score goes up as well, which means the sentiments determined by the dictionary do match the start ratings for the reviews.

Now lets do the same for the AFINN dictionary

```{r message=FALSE, warning=FALSE}
rrSenti_afinn<- rrTokens %>% inner_join(get_sentiments("afinn"), by="word")

revSenti_afinn <- rrSenti_afinn %>%
  group_by(review_id, stars) %>% 
  summarise(nwords=n(), sentiSum =sum(value))

revSenti_afinn %>%
  group_by(stars) %>% 
  summarise(avgLen=mean(nwords), avgSenti=mean(sentiSum)) %>%
  ggplot (aes(x= stars, y=avgSenti)) +geom_bar(stat="identity")

```

Same thing with the afinn dictionary, the results make total sense as the star ratings go up, the average sentiment score goes up which means the sentiments determined by the dictionary do match the start ratings for the reviews in terms of neg/pos reviews. (taking into consideration that higher star ratings are positive and lower star ratings are negative as discussed in the first section of the assignment)


### Predicting high/low reviews based on on aggregated sentiment of words in the reviews

Now, we cold predict if the reviews will be high or low based on the dictionaries without actually building a model. In order to do this, we will consider reviews with start ratings <2 as negative and reviews with star ratings > 4 as positive. Then, if the sentiment is >0 then we will label the pred_hilo as 1 else, it it is <0 will label it as -1. Finally, we will be able to print out our confusion matrix with the predicted and actual and see how each dictionary performs.

Lets start with afinn dictionary

```{r message=FALSE, warning=FALSE}
#we can consider reviews with 1 to 2 stars as positive, and this with 4 to 5 stars as negative
revSenti_afinn <- revSenti_afinn %>% mutate(hiLo=ifelse(stars<=2,-1, ifelse(stars>=4, 1, 0 )))
revSenti_afinn <- revSenti_afinn %>% mutate(pred_hiLo=ifelse(sentiSum >0, 1, -1)) 
#filter out the reviews with 3 stars, and get the confusion matrix for hiLo vs pred_hiLo
xx<-revSenti_afinn %>% filter(hiLo!=0)
table(actual=xx$hiLo, predicted=xx$pred_hiLo )
```

Accuracy from afinn dictionary is: 81.78% which good.

Now lets see how bing dictionary will perform

```{r message=FALSE, warning=FALSE}
revSenti_bing <- revSenti_bing %>% mutate(hiLo=ifelse(stars<=2,-1, ifelse(stars>=4, 1, 0 )))
revSenti_bing <- revSenti_bing %>% mutate(pred_hiLo=ifelse(sentiScore >0, 1, -1)) 
#filter out the reviews with 3 stars, and get the confusion matrix for hiLo vs pred_hiLo
xx<-revSenti_bing %>% filter(hiLo!=0)
table(actual=xx$hiLo, predicted=xx$pred_hiLo )
```

Accuracy from the bing dictionary is: 81.30% which is good but slightly lower than afinn.

Afinn dictionary gave us the best performance.



## (d) Develop models to predict review sentiment. 

In this section, we will develop models to predict review sentiment. We will use three different models on each dictionary. (random forest, SVM and Naive bayes)

### Random Forest model Using Bing

In the code below, we will develop a random forest model to predict HiLo using the bing dictionary. 
We had to split the data into 50% train and 50% test just because the data is very large 

```{r message=FALSE, warning=FALSE}
#Or, since we want to keep the stars column
revDTM_sentiBing <- rrSenti_bing %>%  pivot_wider(id_cols = c(review_id,stars), names_from = word, values_from = tf_idf)  %>% ungroup()

#filter out the reviews with stars=3, and calculate hiLo sentiment 'class'
revDTM_sentiBing <- revDTM_sentiBing %>% filter(stars!=3) %>% mutate(hiLo=ifelse(stars<=2, -1, 1)) %>% select(-stars)

#how many review with 1, -1  'class'
revDTM_sentiBing %>% group_by(hiLo) %>% tally()


#develop a random forest model to predict hiLo from the words in the reviews

library(ranger)

#replace all the NAs with 0
revDTM_sentiBing<-revDTM_sentiBing %>% replace(., is.na(.), 0)

revDTM_sentiBing$hiLo<- as.factor(revDTM_sentiBing$hiLo)


library(rsample)
revDTM_sentiBing_split<- initial_split(revDTM_sentiBing, 0.5)
revDTM_sentiBing_trn<- training(revDTM_sentiBing_split)
revDTM_sentiBing_tst<- testing(revDTM_sentiBing_split)

rfModel1<-ranger(dependent.variable.name = "hiLo", data=revDTM_sentiBing_trn %>% select(-review_id), num.trees = 500, importance='permutation', probability = TRUE)

rfModel1
```

Now that we trained the model, lets look at the performance. First, we will look at the confusion matrix and then compute the accuracy for both train set and test set.
```{r message=FALSE, warning=FALSE}
revSentiBing_predTrn<- predict(rfModel1, revDTM_sentiBing_trn %>% select(-review_id))$predictions
revSentiBing_predTst<- predict(rfModel1, revDTM_sentiBing_tst %>% select(-review_id))$predictions

ConfM_train <- table(actual=revDTM_sentiBing_trn$hiLo, preds=revSentiBing_predTrn[,2]>0.5)
#display confusion Matrix
ConfM_train

#calculate accuracy
RF1_train_accuracy <- sum(diag(ConfM_train))/sum(ConfM_train)
RF1_train_accuracy
```

Now lets look at the confusion matrix and the accuracy on the test set.

```{r message=FALSE, warning=FALSE}
ConfM_test <- table(actual=revDTM_sentiBing_tst$hiLo, preds=revSentiBing_predTst[,2]>0.5)
#print the confusion matirx
ConfM_test
#Calcl Accuracy
RF1_test_accuracy <- sum(diag(ConfM_test))/sum(ConfM_test)
RF1_test_accuracy
```

We could see from the results above that accuracy is at 96% on the train data and at 88% on the test data. There is a little bit of overfit but overall these results are very good.

Now lets look at the ROC curve for both test and train.

```{r message=FALSE, warning=FALSE}
rocTrn <- roc(revDTM_sentiBing_trn$hiLo, revSentiBing_predTrn[,2], levels=c(-1, 1))
rocTst_bing <- roc(revDTM_sentiBing_tst$hiLo, revSentiBing_predTst[,2], levels=c(-1, 1))

plot.roc(rocTrn, col='blue', legacy.axes = TRUE)
plot.roc(rocTst_bing, col='red', add=TRUE)
legend("bottomright", legend=c("Training", "Test"),
        col=c("blue", "red"), lwd=2, cex=0.8, bty='n')
```

We see again that performance on the train data is better then the test data but this ROC indicates good results. 


### Random Forest model Using AFINN

Now lets look at how predicting HiLo with random forest will perform with AFINN.

```{r message=FALSE, warning=FALSE}
#Or, since we want to keep the stars column
revDTM_sentiAfinn <- rrSenti_afinn %>%  pivot_wider(id_cols = c(review_id,stars), names_from = word, values_from = tf_idf)  %>% ungroup()

#filter out the reviews with stars=3, and calculate hiLo sentiment 'class'
revDTM_sentiAfinn<- revDTM_sentiAfinn %>% filter(stars!=3) %>% mutate(hiLo=ifelse(stars<=2, -1, 1)) %>% select(-stars)

#how many review with 1, -1  'class'
revDTM_sentiAfinn %>% group_by(hiLo) %>% tally()

#develop a random forest model to predict hiLo from the words in the reviews

library(ranger)

#replace all the NAs with 0
revDTM_sentiAfinn<-revDTM_sentiAfinn %>% replace(., is.na(.), 0)

revDTM_sentiAfinn$hiLo<- as.factor(revDTM_sentiAfinn$hiLo)


library(rsample)
revDTM_sentiAfinn_split<- initial_split(revDTM_sentiAfinn, 0.5)
revDTM_sentiAfinn_trn<- training(revDTM_sentiAfinn_split)
revDTM_sentiAfinn_tst<- testing(revDTM_sentiAfinn_split)

rfModel2 <-ranger(dependent.variable.name = "hiLo", data=revDTM_sentiAfinn_trn %>% select(-review_id), num.trees = 500, importance='permutation', probability = TRUE)

rfModel2
```

Now that we trained the model, lets look at the performance. First, we will look at the confusion matrix and then compute the accuracy for both train set and test set.
```{r message=FALSE, warning=FALSE}
revSentiAfinn_predTrn<- predict(rfModel2, revDTM_sentiAfinn_trn %>% select(-review_id))$predictions
revSentiAfinn_predTst<- predict(rfModel2, revDTM_sentiAfinn_tst %>% select(-review_id))$predictions

ConfM_train <- table(actual=revDTM_sentiAfinn_trn$hiLo, preds=revSentiAfinn_predTrn[,2]>0.5)
#display confusion Matrix
ConfM_train

#calculate accuracy
RF2_train_accuracy <- sum(diag(ConfM_train))/sum(ConfM_train)
RF2_train_accuracy
```

Now lets look at the confusion matrix and the accuracy on the test set.

```{r message=FALSE, warning=FALSE}
ConfM_test <- table(actual=revDTM_sentiAfinn_tst$hiLo, preds=revSentiAfinn_predTst[,2]>0.5)
#print the confusion matirx
ConfM_test
#Calcl Accuracy
RF2_test_accuracy <- sum(diag(ConfM_test))/sum(ConfM_test)
RF2_test_accuracy
```

We could see from the results above that accuracy is at 94% on the train data and at 86% on the test data. There is a little bit of overfit but overall these results are very good. 

The Bing model performs better than the Afinn model according to the accuracy.
Now lets look at the ROC curves

```{r message=FALSE, warning=FALSE}
rocTrn <- roc(revDTM_sentiAfinn_trn$hiLo, revSentiAfinn_predTrn[,2], levels=c(-1, 1))
rocTst_Afinn <- roc(revDTM_sentiAfinn_tst$hiLo, revSentiAfinn_predTst[,2], levels=c(-1, 1))

plot.roc(rocTrn, col='blue', legacy.axes = TRUE)
plot.roc(rocTst_Afinn, col='red', add=TRUE)
legend("bottomright", legend=c("Training", "Test"),
        col=c("blue", "red"), lwd=2, cex=0.8, bty='n')
```

We see again that performance on the train data is better then the test data but this ROC indicates good results.

Lets compare the ROC curves for the AFINN test set vs the Bing test set

```{r message=FALSE, warning=FALSE}
plot.roc(rocTst_bing, col='blue', legacy.axes = TRUE)
plot.roc(rocTst_Afinn, col='red', add=TRUE)
legend("bottomright", legend=c("Bing", "Afinn"),
        col=c("blue", "red"), lwd=2, cex=0.8, bty='n')

```

We could also see from the ROC curve that the Bing dictionary performs better when trying to predict HiLo of reviews through a Random Forest model.

### Naive-Bayes model using Bing dictionary

Now lets use a different model to predict HiLo. We will use the Naive-bayes model for both Bing and Afinn and then compare the results at the end.

```{r message=FALSE, warning=FALSE}
nbModel1<-naiveBayes(hiLo ~ ., data=revDTM_sentiBing_trn %>% select(-review_id))

revSentiBing_NBpredTrn<-predict(nbModel1, revDTM_sentiBing_trn, type = "raw")
revSentiBing_NBpredTst<-predict(nbModel1, revDTM_sentiBing_tst, type = "raw")

```

Now lets look at the AUC value for the train set:

```{r message=FALSE, warning=FALSE}
auc(as.numeric(revDTM_sentiBing_trn$hiLo), revSentiBing_NBpredTrn[,2])
```

And the AUC value on the test set

```{r message=FALSE, warning=FALSE}
auc(as.numeric(revDTM_sentiBing_tst$hiLo), revSentiBing_NBpredTst[,2])
```

We see the AUC value for the Bing Naive base model on the test set is 72.43% which is good. 


### Naive-Bayes model using Afinn dictionary

Now lets compare to how the AFINN dictionary will perform using the Naive base model

```{r message=FALSE, warning=FALSE}
nbModel2<-naiveBayes(hiLo ~ ., data=revDTM_sentiAfinn_trn %>% select(-review_id))

revSentiAfinn_NBpredTrn<-predict(nbModel2, revDTM_sentiAfinn_trn, type = "raw")
revSentiAfinn_NBpredTst<-predict(nbModel2, revDTM_sentiAfinn_tst, type = "raw")
```

Now lets look at the AUC value for the train set:

```{r message=FALSE, warning=FALSE}
auc(as.numeric(revDTM_sentiAfinn_trn$hiLo), revSentiAfinn_NBpredTrn[,2])
```

And the AUC value on the test set

```{r message=FALSE, warning=FALSE}
auc(as.numeric(revDTM_sentiAfinn_tst$hiLo), revSentiAfinn_NBpredTst[,2])
```

We see the AUC value for the Afinn dictionary on the test set is 74.12%. So using the Naive bayes model, the Afinn dictionary gives us better predictions.

### SVM Model using Bing Dictionary 

Now lets try our 3rd model: SVM classification â€“ for restaurant reviews.

First, we will build the SVM model using the data from the bing dictionary and then we will build the same using the AFINN dictionary and compare the results

```{r message=FALSE, warning=FALSE}
system.time( svmM1 <- svm(as.factor(hiLo) ~., data = revDTM_sentiBing_trn
%>% select(-review_id), kernel="radial", cost=5, gamma=5, scale=FALSE) )

revDTM_predTrn_svm1<-predict(svmM1, revDTM_sentiBing_trn)
revDTM_predTst_svm1<-predict(svmM1, revDTM_sentiBing_tst)

```

Now lets look at the performance of this model. We will print out the confusion matrix for the train data and the test data then calculate the accuracy for both

```{r message=FALSE, warning=FALSE}
#Condusion Matrix on train data
cm_svm_train <- table(actual= revDTM_sentiBing_trn$hiLo, predicted= revDTM_predTrn_svm1)
cm_svm_train
#Calculate accuracy
svm1_train_accuracy <- sum(diag(cm_svm_train))/sum(cm_svm_train)
svm1_train_accuracy
```

We get an accuracy of 96,57%

Now lets see the performance on the test data

```{r message=FALSE, warning=FALSE}
#Condusion Matrix on train data
cm_svm_test <- table(actual= revDTM_sentiBing_tst$hiLo, predicted= revDTM_predTst_svm1)
cm_svm_test
#Calculate accuracy
svm1_test_accuracy <- sum(diag(cm_svm_test))/sum(cm_svm_test)
svm1_test_accuracy
```

We get accuracy of 87.84% on the test set which is good. We do have a little bit of overfit but the results are still good overall.

We could still tune the SVM parameters using the tune function in SVM. lets experiment with it and see if we get even better results. The code below with give us the best SVM model; However, since it takes hours to run and causes trouble on the knitted RMD, we will just stick with the model above.

```{r message=FALSE, warning=FALSE, eval=FALSE}
system.time( svm_tune <- tune(svm, as.factor(hiLo) ~., data = revDTM_sentiBing_trn %>% select(-review_id),
kernel="radial", ranges = list( cost=c(0.1,1,10,50), gamma = c(0.5,1,2,5, 10))) )

#Check performance for different tuned parameters
svm_tune$performances
#Best model
svm_tune$best.parameters
svm_tune$best.model
```

### SVM Model using Afinn Dictionary 

Now lets build the SVM model using the Afinn dictionary and compare the results to the bing SVM model

```{r message=FALSE, warning=FALSE}
system.time( svmM2 <- svm(as.factor(hiLo) ~., data = revDTM_sentiAfinn_trn
%>% select(-review_id), kernel="radial", cost=5, gamma=5, scale=FALSE) )

revDTM_predTrn_svm2<-predict(svmM2, revDTM_sentiAfinn_trn)
revDTM_predTst_svm2<-predict(svmM2, revDTM_sentiAfinn_tst)

```

Now lets look at the performance of this model. We will print out the confusion matrix for the train data and the test data then calculate the accuracy for both

```{r message=FALSE, warning=FALSE}
#Condusion Matrix on train data
cm_svm2_train <- table(actual= revDTM_sentiAfinn_trn$hiLo, predicted= revDTM_predTrn_svm2)
cm_svm2_train
#Calculate accuracy
svm2_train_accuracy <- sum(diag(cm_svm2_train))/sum(cm_svm2_train)
svm2_train_accuracy
```

We get an accuracy of 93.08%

Now lets see the performance on the test data

```{r message=FALSE, warning=FALSE}
#Condusion Matrix on train data
cm_svm2_test <- table(actual= revDTM_sentiAfinn_tst$hiLo, predicted= revDTM_predTst_svm2)
cm_svm2_test
#Calculate accuracy
svm2_test_accuracy <- sum(diag(cm_svm2_test))/sum(cm_svm2_test)
svm2_test_accuracy
```

We get accuracy of 86.47% on the test set which is good. We do have a little bit of overfit but the results are still good overall.

So we could see that using Naive base model to predict review sentiment gives better results with the Bing dictionary based on the accuracy performance.


### Develop a model on broader set of terms

In this section, we will develop a model using a broader set of terms and not just the terms we have in each dictionary. In order to do this, we will use the rrtokens directly. We will then do some clean-up activities like removing words that are found in > 90% of the reviews or <30% of the reviews. 

We will use a random forest ranger model for this section

### Data Prep

```{r message=FALSE, warning=FALSE}
#in how many reviews each word occurs
rWords<-rrTokens %>% group_by(word) %>% summarise(nr=n()) %>% arrange(desc(nr))

#delete the words that are in more than 90% of less than 30% of reviews
reduced_rWords<-rWords %>% filter(nr< 6000 & nr > 30)
#Store the result bach in rrtokens
reduced_rrTokens <- left_join(reduced_rWords, rrTokens)

#Now convert it to a DTM, where each row is for a review (document), and columns are the terms (words)
revDTM  <- reduced_rrTokens %>%  pivot_wider(id_cols = c(review_id,stars), names_from = word, values_from = tf_idf)  %>% ungroup()

#create the dependent variable hiLo of good/bad reviews absed on stars, and remove the review with stars=3
revDTM <- revDTM %>% filter(stars!=3) %>% mutate(hiLo=ifelse(stars<=2, -1, 1)) %>% select(-stars)

#replace NAs with 0s
revDTM<-revDTM %>% replace(., is.na(.), 0)

revDTM$hiLo<-as.factor(revDTM$hiLo)

revDTM_split<- initial_split(revDTM, 0.4)
revDTM_trn<- training(revDTM_split)
revDTM_tst<- testing(revDTM_split)
```

Now that we prepared our data, we could run the random forest model

```{r message=FALSE, warning=FALSE}
rfModel2<-ranger(dependent.variable.name = "hiLo", data=revDTM_trn %>% select(-review_id), num.trees = 500, importance='permutation', probability = TRUE)

rfModel2
```

We could see from above that oob error is very low. which means this model is also performing well.

### Discussion

in this section, we will summurize what we did for questions (c) amd (d).

First, in question in (c), We saw which words contributed to to positive/negative senteiment according to the three lexicons: Bing, Afinn, and NCR. We saw results that made sense for Afinn and Bing; However, for NCR, we saw some words that don't necessarily belong to negative or positive sentiments like (food, chickenm restaurant,etc). This was expected as the NCF dictionary uses emotions and not just negative and positive sentiments.

Then, we performed analysis by review sentiment for Bing and Afinn to look into sentiment by review and see how it relates to review's star ratings. At this point, we are not developing a model, but we are predicting the sentiment (positive or negative) based on the results we get from the dictionaries. We classified reviews on high/low stats based on aggregated sentiment of words in the reviews. This enabled us to get the actual (based on start ratings <2 negative >2 positive) and the predicted (based on the dictionaries). We printed the confusion matrices for both Afinn and Bing dictionaries and found the following performances:

  - Bing accuracy in predicting review sentiment: 81.30%
  - Afinn accuracy in predicting review sentiment: 81.78%

As you can see, Afinn performed slightly better.

In question (d) we developed 3 different models using the data from both Afinn and bing dictionaries to predict review sentiment. We developed the following models:

  - Random forest (Bing dictionary)
  - Random forest (Afinn dictionary)
  - Naive Base (bing Dictionary)
  - Naive Base (Afinn Dictionary)
  - SVM (bing Dictionary)
  - SVM (Afinn dictionary)

Before we started building our models, we performed lemmatization on both the Afinn words and the bing words. Lemmitzation is the process of bring words back to their 'original form' for example, words like building, builds, built will be transformed to build.

All the models performed well in terms of accuracy and ROC. The accuracy was general from 84% to 88% on the test data and the AUC was between (0.70 and 0.73) on the test data. We also noticed the models (Random forest, Naive base, and SVM) perform better than the predictions in question (c) (only based on the dictionary prediction: no models developed).

Finally, we developed a model using a broader list of terms that are not restricted just to the dictionary terms only. We developed a random forest model for this purpose and used the data from rrtokens as it has all the words from the reviews. We had to do some data preparations before running the model like removing words that are used in more than 90% of the reviews or less than 30%, created the hiLo field (<2 stars negative >2 stars positive), and of course replaced all na values. Finally, we ran the random forest model and got high accuracy rate.




